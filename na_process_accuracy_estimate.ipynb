{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d9c2971d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9c2971d",
        "outputId": "7e2f341f-d3a3-48cc-9618-e209a6cdbafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "import copy\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c9bf30de",
      "metadata": {
        "id": "c9bf30de"
      },
      "outputs": [],
      "source": [
        "# 시드 고정\n",
        "import os\n",
        "\n",
        "SEED=42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "735b50d0",
      "metadata": {
        "id": "735b50d0"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"train.csv\") # 학습용 데이터\n",
        "test = pd.read_csv(\"submission.csv\") # 테스트 데이터(제출파일의 데이터)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#india_count = train['customer_country'].str.contains('india', case=False).sum()\n",
        "#india_count_st = test['customer_country'].str.contains('india', case=False).sum()\n",
        "\n",
        "#print(\"India가 포함된 행의 수 : \", india_count)\n",
        "#print(\"India가 포함된 행의 수 from test : \",india_count_st)"
      ],
      "metadata": {
        "id": "n9CDP9eVCJid"
      },
      "id": "n9CDP9eVCJid",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "af1dc99b",
      "metadata": {
        "id": "af1dc99b"
      },
      "outputs": [],
      "source": [
        "# 학습 타겟 데이터\n",
        "targets=train['is_converted']\n",
        "rows=train.shape[0]\n",
        "\n",
        "with open('permitted.json','r') as f:\n",
        "    permitted=json.load(f)\n",
        "\n",
        "#\n",
        "train=train.drop('is_converted',axis=1)\n",
        "test=test.drop('id',axis=1)\n",
        "test=test.drop('is_converted',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8ee07045",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ee07045",
        "outputId": "a3c4d8b1-caa1-444f-c74d-01248f0b8eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "numerical columns: 13\n",
            "categorical columns: 15\n",
            "total columns: 28\n"
          ]
        }
      ],
      "source": [
        "# columns\n",
        "cols_by_type={}\n",
        "\n",
        "cols_by_type['categorical']=train.columns[train.dtypes=='object'].tolist()\n",
        "cols_by_type['numerical']=train.columns[train.dtypes!='object'].tolist()\n",
        "#colsByType['numerical'].remove('is_converted')\n",
        "\n",
        "print('\\nnumerical columns: '+str(len(cols_by_type['numerical'])))\n",
        "print('categorical columns: '+str(len(cols_by_type['categorical'])))\n",
        "print('total columns: '+str(len(cols_by_type['numerical'])+len(cols_by_type['categorical'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8c96acdc",
      "metadata": {
        "id": "8c96acdc"
      },
      "outputs": [],
      "source": [
        "# delete cols\n",
        "del_cols=['ver_win_rate_x','ver_win_ratio_per_bu',\n",
        "          'business_subarea', 'product_subcategory', 'product_modelname',\n",
        "          'customer_country.1']\n",
        "\n",
        "# preserve\n",
        "# preserve=pd.DataFrame()\n",
        "# preserve['com_reg_ver_win_rate']=total_data['com_reg_ver_win_rate']\n",
        "\n",
        "train_process=train.drop(del_cols,axis=1)\n",
        "test_process=test.drop(del_cols,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4328e1d8",
      "metadata": {
        "id": "4328e1d8"
      },
      "outputs": [],
      "source": [
        "# id_strategic_ver it_strategic_ver idit_strategic_ver\n",
        "ver=['id_strategic_ver', 'it_strategic_ver', 'idit_strategic_ver']\n",
        "train_process['strategic_ver']=np.where(train_process['idit_strategic_ver']>0,1,0)\n",
        "test_process['strategic_ver']=np.where(test_process['idit_strategic_ver']>0,1,0)\n",
        "train_process=train_process.drop(ver,axis=1)\n",
        "test_process=test_process.drop(ver,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_countries = ['india', 'brazil', 'unitedstates', 'mexico', 'philippines', 'colombia', 'u.a.e', 'unitedkingdom', 'saudiarabia', 'chile', 'italy', 'peru']\n",
        "\n",
        "for country in selected_countries:\n",
        "    country_count = train_process['customer_country'].str.contains(country, case=False).sum()\n",
        "    country_count_st = test_process['customer_country'].str.contains(country, case=False).sum()\n",
        "    print(f\"{country.capitalize()}가 포함된 행의 수 (Train): {country_count}\")\n",
        "    print(f\"{country.capitalize()}가 포함된 행의 수 (Test): {country_count_st}\")\n",
        "#Kfold후\n",
        "#brazil                  1045 - 0\n",
        "#india                    906 - 2\n",
        "#unitedstates             782\n",
        "#philippines              289 - 0\n",
        "#OT                       254\n",
        "#peru                     211 - 2\n",
        "#mexico                   180 - 0\n",
        "#colombia                 127 - 1\n",
        "#u.a.e                    120\n",
        "#italy                    120\n",
        "#chile                    108\n",
        "#saudiarabia              102\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMgHCwCbhYaq",
        "outputId": "7dd7f5b2-cbff-4220-87a2-1c76691334aa"
      },
      "id": "bMgHCwCbhYaq",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India가 포함된 행의 수 (Train): 16924\n",
            "India가 포함된 행의 수 (Test): 908\n",
            "Brazil가 포함된 행의 수 (Train): 8739\n",
            "Brazil가 포함된 행의 수 (Test): 1045\n",
            "Unitedstates가 포함된 행의 수 (Train): 0\n",
            "Unitedstates가 포함된 행의 수 (Test): 0\n",
            "Mexico가 포함된 행의 수 (Train): 2771\n",
            "Mexico가 포함된 행의 수 (Test): 180\n",
            "Philippines가 포함된 행의 수 (Train): 2570\n",
            "Philippines가 포함된 행의 수 (Test): 289\n",
            "Colombia가 포함된 행의 수 (Train): 1995\n",
            "Colombia가 포함된 행의 수 (Test): 128\n",
            "U.a.e가 포함된 행의 수 (Train): 1785\n",
            "U.a.e가 포함된 행의 수 (Test): 129\n",
            "Unitedkingdom가 포함된 행의 수 (Train): 0\n",
            "Unitedkingdom가 포함된 행의 수 (Test): 0\n",
            "Saudiarabia가 포함된 행의 수 (Train): 0\n",
            "Saudiarabia가 포함된 행의 수 (Test): 0\n",
            "Chile가 포함된 행의 수 (Train): 1234\n",
            "Chile가 포함된 행의 수 (Test): 108\n",
            "Italy가 포함된 행의 수 (Train): 1148\n",
            "Italy가 포함된 행의 수 (Test): 120\n",
            "Peru가 포함된 행의 수 (Train): 1149\n",
            "Peru가 포함된 행의 수 (Test): 213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2597e0d5",
      "metadata": {
        "id": "2597e0d5"
      },
      "outputs": [],
      "source": [
        "# country columns\n",
        "# region\n",
        "def preprocess_region(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        if permitted.get(x):\n",
        "            return permitted[x]\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "def response_corporate_encoding(train_data):\n",
        "    permit={}\n",
        "\n",
        "    for train_label in train_data.value_counts().index:\n",
        "        permit[train_label]=1\n",
        "    permit['OT']=1\n",
        "\n",
        "    return permit\n",
        "\n",
        "def preprocess_response_corporate(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        if permitted.get(x):\n",
        "            return x\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def country_encoding(train_data):\n",
        "    permit={}\n",
        "\n",
        "    train_labels=train_data.apply(lambda x:x.lower().replace(' ','').replace('/',' ').split(' ')[-1] if type(x)==type('') else np.nan).value_counts()\n",
        "    train_labels=sorted(train_labels.items(),key=lambda x:x[1],reverse=True)\n",
        "\n",
        "    for train_label in train_labels[:100]:\n",
        "        if train_label[0]:\n",
        "            permit[f'{train_label[0]}']=1\n",
        "\n",
        "    if permit.get(''):\n",
        "        del permit['']\n",
        "\n",
        "    #\n",
        "    permit['OT']='OT'\n",
        "    permit['dump_key']='dump_value'\n",
        "    permit['türkiye']='turkey'\n",
        "    del permit['700patrooncreekblvdalbanyny12206']\n",
        "    del permit['uaedubai']\n",
        "\n",
        "    return permit\n",
        "\n",
        "def preprocess_customer_country(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        x=x.lower().replace(' ','').replace('/',' ')\n",
        "        for word in x.split(' '):\n",
        "            if permitted.get(word):\n",
        "                return word\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "# region\n",
        "train_process['region']=train_process['response_corporate'].apply(lambda x:preprocess_region(x,permitted=permitted['region']))\n",
        "test_process['region']=test_process['response_corporate'].apply(lambda x:preprocess_region(x,permitted=permitted['region']))\n",
        "\n",
        "# response_corporate\n",
        "permitted['response_corporate']=response_corporate_encoding(train['response_corporate'])\n",
        "train_process['response_corporate']=train_process['response_corporate'].apply(lambda x:preprocess_response_corporate(x,permitted=permitted['response_corporate']))\n",
        "test_process['response_corporate']=test_process['response_corporate'].apply(lambda x:preprocess_response_corporate(x,permitted=permitted['response_corporate']))\n",
        "\n",
        "# customer_country\n",
        "permitted['customer_country']=country_encoding(train['customer_country'])\n",
        "train_process['customer_country']=train_process['customer_country'].apply(lambda x:preprocess_customer_country(x,permitted=permitted['customer_country']))\n",
        "test_process['customer_country']=test_process['customer_country'].apply(lambda x:preprocess_customer_country(x,permitted=permitted['customer_country']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#region_value_counts = train_process['customer_country'].value_counts()\n",
        "#print(region_value_counts)\n",
        "\n",
        "selected_countries = ['india', 'brazil', 'unitedstates', 'mexico', 'philippines', 'colombia', 'u.a.e', 'unitedkingdom', 'saudiarabia', 'chile', 'italy', 'peru']\n",
        "\n",
        "for country in selected_countries:\n",
        "    country_count = train_process['customer_country'].str.contains(country, case=False).sum()\n",
        "    country_count_st = test_process['customer_country'].str.contains(country, case=False).sum()\n",
        "    print(f\"{country.capitalize()}가 포함된 행의 수 (Train): {country_count}\")\n",
        "    print(f\"{country.capitalize()}가 포함된 행의 수 (Test): {country_count_st}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjQQXnijiZpq",
        "outputId": "fbd484c5-0d98-4f89-cc0c-44f4caac2c5e"
      },
      "id": "HjQQXnijiZpq",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India가 포함된 행의 수 (Train): 16880\n",
            "India가 포함된 행의 수 (Test): 906\n",
            "Brazil가 포함된 행의 수 (Train): 8737\n",
            "Brazil가 포함된 행의 수 (Test): 1045\n",
            "Unitedstates가 포함된 행의 수 (Train): 3794\n",
            "Unitedstates가 포함된 행의 수 (Test): 782\n",
            "Mexico가 포함된 행의 수 (Train): 2756\n",
            "Mexico가 포함된 행의 수 (Test): 180\n",
            "Philippines가 포함된 행의 수 (Train): 2567\n",
            "Philippines가 포함된 행의 수 (Test): 289\n",
            "Colombia가 포함된 행의 수 (Train): 1985\n",
            "Colombia가 포함된 행의 수 (Test): 127\n",
            "U.a.e가 포함된 행의 수 (Train): 1624\n",
            "U.a.e가 포함된 행의 수 (Test): 120\n",
            "Unitedkingdom가 포함된 행의 수 (Train): 1488\n",
            "Unitedkingdom가 포함된 행의 수 (Test): 43\n",
            "Saudiarabia가 포함된 행의 수 (Train): 1472\n",
            "Saudiarabia가 포함된 행의 수 (Test): 102\n",
            "Chile가 포함된 행의 수 (Train): 1228\n",
            "Chile가 포함된 행의 수 (Test): 108\n",
            "Italy가 포함된 행의 수 (Train): 1142\n",
            "Italy가 포함된 행의 수 (Test): 120\n",
            "Peru가 포함된 행의 수 (Train): 1134\n",
            "Peru가 포함된 행의 수 (Test): 211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f11278d9",
      "metadata": {
        "id": "f11278d9"
      },
      "outputs": [],
      "source": [
        "# business_unit\n",
        "train_process['business_unit']=train_process['business_unit'].replace('Solution','ETC')\n",
        "train_process['business_unit']=train_process['business_unit'].replace('CM','ETC')\n",
        "\n",
        "test_process['business_unit']=test_process['business_unit'].replace('Solution','ETC')\n",
        "test_process['business_unit']=test_process['business_unit'].replace('CM','ETC')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type_value_counts = train_process['customer_type'].value_counts()\n",
        "print(type_value_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmPEY4TAnOhw",
        "outputId": "01379d28-3a31-4674-bea2-2a69f01087ef"
      },
      "id": "DmPEY4TAnOhw",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End-Customer                    6647\n",
            "End Customer                    3996\n",
            "Specifier/ Influencer           2525\n",
            "Channel Partner                 1368\n",
            "Service Partner                  349\n",
            "Solution Eco-Partner             146\n",
            "Installer/Contractor              52\n",
            "Specifier / Influencer            43\n",
            "Corporate                         31\n",
            "HVAC Engineer                     23\n",
            "Engineer                          20\n",
            "Developer                         17\n",
            "Technician                        16\n",
            "Consultant                        15\n",
            "Other                             10\n",
            "Home Owner                        10\n",
            "End-user                           8\n",
            "Manager / Director                 8\n",
            "Software/Solution Provider         7\n",
            "Etc.                               6\n",
            "Architect/Consultant               5\n",
            "Homeowner                          5\n",
            "Reseller                           5\n",
            "Installer                          5\n",
            "Interior Designer                  5\n",
            "Distributor                        4\n",
            "Others                             4\n",
            "Dealer/Distributor                 2\n",
            "System Integrator                  2\n",
            "Software / Solution Provider       1\n",
            "Technical Assistant                1\n",
            "Commercial end-user                1\n",
            "Administrator                      1\n",
            "Name: customer_type, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_type = [\"endcustomer\",\"Specifier / Influencer\",\"channelpartner\"]\n",
        "\n",
        "for ty in selected_type:\n",
        "    typ = train_process['customer_type'].str.contains(ty, case=False).sum()\n",
        "    typ_st = test_process['customer_type'].str.contains(ty, case=False).sum()\n",
        "    print(f\"{country.capitalize()}가 포함된 행의 수 (Train): {ty}\")\n",
        "    print(f\"{country.capitalize()}가 포함된 행의 수 (Test): {typ_st}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0CxnshfnoS3",
        "outputId": "c0104368-fae7-45d0-bc6d-395b9b0ed74a"
      },
      "id": "o0CxnshfnoS3",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peru가 포함된 행의 수 (Train): endcustomer\n",
            "Peru가 포함된 행의 수 (Test): 0\n",
            "Peru가 포함된 행의 수 (Train): Specifier / Influencer\n",
            "Peru가 포함된 행의 수 (Test): 0\n",
            "Peru가 포함된 행의 수 (Train): channelpartner\n",
            "Peru가 포함된 행의 수 (Test): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fdf19be8",
      "metadata": {
        "id": "fdf19be8"
      },
      "outputs": [],
      "source": [
        "# customer_type\n",
        "def preprocess_customer_type(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        x=x.lower().replace('-','').replace(' ','')\n",
        "        if permitted.get(x):\n",
        "            return permitted[x]\n",
        "        else:\n",
        "            return 'OT'\n",
        "    return x\n",
        "\n",
        "train_process['customer_type']=train_process['customer_type'].apply(lambda x:preprocess_customer_type(x,permitted=permitted['customer_type']))\n",
        "test_process['customer_type']=test_process['customer_type'].apply(lambda x:preprocess_customer_type(x,permitted=permitted['customer_type']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type_value_counts = train_process['customer_type'].value_counts()\n",
        "print(type_value_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JBFkhAKotqj",
        "outputId": "a8a3bcbd-1cb4-47a2-ecd6-708bd34c0674"
      },
      "id": "-JBFkhAKotqj",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EC    10643\n",
            "SI     2568\n",
            "CP     1368\n",
            "OT      759\n",
            "Name: customer_type, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "76561af9",
      "metadata": {
        "id": "76561af9"
      },
      "outputs": [],
      "source": [
        "# business_area\n",
        "# total_data['business_area']=total_data['business_area'].replace('hospital & health care','ETC')\n",
        "# total_data['business_area']=total_data['business_area'].replace('factory','ETC')\n",
        "# total_data['business_area']=total_data['business_area'].replace('government department','ETC')\n",
        "# total_data['business_area']=total_data['business_area'].replace('public facility','ETC')\n",
        "# total_data['business_area']=total_data['business_area'].replace('transportation','ETC')\n",
        "# total_data['business_area']=total_data['business_area'].replace('power plant / renewable energy','ETC')\n",
        "train_process['business_area']=train_process['business_area'].fillna('UNK')\n",
        "test_process['business_area']=test_process['business_area'].fillna('UNK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0adb592e",
      "metadata": {
        "id": "0adb592e"
      },
      "outputs": [],
      "source": [
        "# ver_cus, ver_pro\n",
        "grant=['ver_cus', 'ver_pro']\n",
        "train_process['grant_weight']=np.where(train_process['ver_cus']>0,1,0)\n",
        "train_process['grant_weight']=np.where(train_process['ver_pro']>0,1,train_process['grant_weight'])\n",
        "train_process=train_process.drop(grant,axis=1)\n",
        "\n",
        "test_process['grant_weight']=np.where(test_process['ver_cus']>0,1,0)\n",
        "test_process['grant_weight']=np.where(test_process['ver_pro']>0,1,test_process['grant_weight'])\n",
        "test_process=test_process.drop(grant,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0b5ba423",
      "metadata": {
        "id": "0b5ba423"
      },
      "outputs": [],
      "source": [
        "# expected_timeline\n",
        "def preprocess_expected_timeline(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        x=x.lower().replace(' ','').replace('_','')\n",
        "        if permitted.get(x):\n",
        "            return permitted[x]\n",
        "        return 'OT'\n",
        "    return x\n",
        "\n",
        "train_process['expected_timeline']=train_process['expected_timeline'].apply(lambda x:preprocess_expected_timeline(x,permitted=permitted['expected_timeline']))\n",
        "test_process['expected_timeline']=test_process['expected_timeline'].apply(lambda x:preprocess_expected_timeline(x,permitted=permitted['expected_timeline']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c9a83af6",
      "metadata": {
        "id": "c9a83af6"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "\n",
        "# lead_desc_length, historical_existing_cnt\n",
        "numerical=['lead_desc_length','historical_existing_cnt']\n",
        "#total_data[numerical]=scaler.fit_transform(total_data[numerical])\n",
        "train_process[numerical]=np.log1p(train_process[numerical])\n",
        "test_process[numerical]=np.log1p(test_process[numerical])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "14fc77d4",
      "metadata": {
        "id": "14fc77d4"
      },
      "outputs": [],
      "source": [
        "# inquiry_type\n",
        "def preprocess_inquiry_type(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        x=x.lower().replace('_',' ')\n",
        "        for word in x.split(' '):\n",
        "            if permitted.get(word):\n",
        "                return permitted[word]\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "train_process['inquiry_type']=train_process['inquiry_type'].apply(lambda x:preprocess_inquiry_type(x,permitted=permitted['inquiry_type']))\n",
        "test_process['inquiry_type']=test_process['inquiry_type'].apply(lambda x:preprocess_inquiry_type(x,permitted=permitted['inquiry_type']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "19e23ecb",
      "metadata": {
        "id": "19e23ecb"
      },
      "outputs": [],
      "source": [
        "# customer_job\n",
        "def preprocess_customer_job(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        porter=PorterStemmer()\n",
        "        tokens=word_tokenize(x)\n",
        "        stems=[porter.stem(token) for token in tokens]\n",
        "        for stem in stems:\n",
        "            if permitted.get(stem):\n",
        "                return permitted[stem]\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "train_process['customer_job']=train_process['customer_job'].apply(lambda x:preprocess_customer_job(x,permitted=permitted['customer_job']))\n",
        "test_process['customer_job']=test_process['customer_job'].apply(lambda x:preprocess_customer_job(x,permitted=permitted['customer_job']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c9baeace",
      "metadata": {
        "id": "c9baeace"
      },
      "outputs": [],
      "source": [
        "# product_category\n",
        "def preprocess_product_category(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        porter=PorterStemmer()\n",
        "        tokens=word_tokenize(x)\n",
        "        stems=[porter.stem(token) for token in tokens]\n",
        "\n",
        "        prefer={}\n",
        "        for pf in permitted['dump_key']:\n",
        "            prefer[pf]=0\n",
        "\n",
        "        for stem in stems:\n",
        "            if permitted.get(stem):\n",
        "                prefer[permitted[stem]]=1\n",
        "\n",
        "        for pf in permitted['dump_key']:\n",
        "            if prefer[pf]>0:\n",
        "                return pf\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "train_process['product_category']=train_process['product_category'].apply(lambda x:preprocess_product_category(x,permitted=permitted['product_category']))\n",
        "test_process['product_category']=test_process['product_category'].apply(lambda x:preprocess_product_category(x,permitted=permitted['product_category']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e576c82d",
      "metadata": {
        "id": "e576c82d"
      },
      "outputs": [],
      "source": [
        "# customer_poisition\n",
        "def preprocess_customer_position(x,permitted):\n",
        "    if type(x)==type(''):\n",
        "        x=x.lower().replace('-',' ').replace('/',' ')\n",
        "        for word in x.split(' '):\n",
        "            if permitted.get(word):\n",
        "                return permitted[word]\n",
        "        return 'OT'\n",
        "    return np.nan\n",
        "\n",
        "train_process['customer_position']=train_process['customer_position'].apply(lambda x:preprocess_customer_position(x,permitted=permitted['customer_position']))\n",
        "test_process['customer_position']=test_process['customer_position'].apply(lambda x:preprocess_customer_position(x,permitted=permitted['customer_position']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cbea62ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbea62ec",
        "outputId": "843b76e4-0d01-4065-acb1-703d16d67edb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'customer_country': Index(['india', 'brazil', 'unitedstates', 'OT', 'mexico', 'philippines',\n",
              "        'colombia', 'u.a.e', 'unitedkingdom', 'saudiarabia', 'chile', 'italy',\n",
              "        'peru', 'germany', 'poland', 'egypt', 'vietnam', 'spain', 'argentina',\n",
              "        'hongkong', 'australia', 'panama', 'france', 'canada', 'turkey',\n",
              "        'ecuador', 'indonesia', 'türkiye', 'singapore', 'southafrica', 'iraq',\n",
              "        'nigeria', 'thailand', 'hungary', 'portugal', 'kenya', 'malaysia',\n",
              "        'bulgaria', 'costarica', 'dominicanrepublic', 'israel', 'oman',\n",
              "        'elsalvador', 'pakistan', 'guatemala', 'kuwait', 'bangladesh', 'qatar',\n",
              "        'switzerland', 'china', 'bolivia', 'honduras', 'lebanon', 'taiwan',\n",
              "        'netherlands', 'belgium', 'bahrain', 'venezuela', 'puertorico',\n",
              "        'greece', 'japan', 'afghanistan', 'algeria', 'morocco', 'romania',\n",
              "        'ghana', 'jordan', 'croatia', 'nicaragua', 'ireland', 'maldives',\n",
              "        'serbia', 'srilanka', 'uruguay', 'albania', 'jamaica', 'southkorea',\n",
              "        'sweden', 'anguilla', 'paraguay', 'malta', 'azerbaijan', 'russia',\n",
              "        'cambodia', 'mozambique', 'yemen', 'bosniaandherzegovina', 'zimbabwe',\n",
              "        'iran', 'slovenia', 'ethiopia', 'botswana', 'papuanewguinea', 'senegal',\n",
              "        'denmark', 'angola', 'uganda', 'barbados'],\n",
              "       dtype='object'),\n",
              " 'business_unit': Index(['ID', 'AS', 'IT', 'ETC'], dtype='object'),\n",
              " 'customer_job': Index(['OT', 'EN', 'AD', 'ED', 'SA', 'PA', 'OP', 'IF', 'AT', 'BS', 'MA'], dtype='object'),\n",
              " 'inquiry_type': Index(['QP', 'SA', 'OT'], dtype='object'),\n",
              " 'product_category': Index(['OT', 'SG', 'VR', 'MS', 'SS', 'TV'], dtype='object'),\n",
              " 'customer_position': Index(['NO', 'OT', 'MA', 'FD', 'DR', 'AN', 'PA', 'EN'], dtype='object'),\n",
              " 'response_corporate': Index(['LGEIL', 'LGESP', 'LGEUS', 'LGEMS', 'LGEPH', 'LGEGF', 'LGECB', 'LGEUK',\n",
              "        'LGESJ', 'LGECL', 'LGEPS', 'LGEIS', 'LGEPR', 'LGEDG', 'LGEPL', 'LGEEG',\n",
              "        'LGEVH', 'LGEES', 'LGETK', 'LGEAR', 'LGEKR', 'LGEHK', 'LGEAP', 'LGESL',\n",
              "        'LGEMK', 'LGEFS', 'LGEAF', 'LGEIN', 'LGELF', 'LGESA', 'LGECI', 'LGETH',\n",
              "        'LGEEF', 'LGEPT', 'LGEML', 'LGEBN', 'LGEYK', 'LGECH', 'LGEHS', 'LGETT',\n",
              "        'LGEJP', 'LGEAS', 'LGESW', 'LGEMC', 'LGERO', 'LGEEB', 'LGEAG', 'LGERA',\n",
              "        'LGECZ', 'LGELA', 'LGEUR', 'LGEIR', 'LGEBT'],\n",
              "       dtype='object'),\n",
              " 'customer_type': Index(['EC', 'SI', 'CP', 'OT'], dtype='object'),\n",
              " 'enterprise': Index(['Enterprise', 'SMB'], dtype='object'),\n",
              " 'expected_timeline': Index(['L3', '36', 'MY', '91', '69', 'OT'], dtype='object'),\n",
              " 'business_area': Index(['UNK', 'corporate / office', 'retail', 'hotel & accommodation',\n",
              "        'special purpose', 'residential (home)', 'education',\n",
              "        'hospital & health care', 'factory', 'government department',\n",
              "        'public facility', 'transportation', 'power plant / renewable energy'],\n",
              "       dtype='object'),\n",
              " 'region': Index(['AP', 'LA', 'NA', 'EU', 'OT'], dtype='object')}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# one hot encoding columns\n",
        "origin_data=train_process.drop('com_reg_ver_win_rate',axis=1)\n",
        "origin_columns=origin_data.columns.to_list()\n",
        "object_columns=origin_data.columns.to_list()\n",
        "object_columns.remove('bant_submit')\n",
        "object_columns.remove('historical_existing_cnt')\n",
        "object_columns.remove('lead_desc_length')\n",
        "object_columns.remove('strategic_ver')\n",
        "object_columns.remove('grant_weight')\n",
        "object_columns.remove('customer_idx')\n",
        "object_columns.remove('lead_owner')\n",
        "for col in object_columns:\n",
        "    permitted['values'][col]=train_process[col].value_counts().index\n",
        "permitted['values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dad8980e",
      "metadata": {
        "id": "dad8980e"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "class Encoder():\n",
        "    def __init__(self):\n",
        "        self.classes=[]\n",
        "\n",
        "    def fit(self,data):\n",
        "        for value in data.value_counts().index:\n",
        "            self.classes.append(value)\n",
        "\n",
        "    def transform(self,data):\n",
        "        result=data.copy(deep=True)\n",
        "        for i,value in enumerate(self.classes):\n",
        "            result=result.replace(value,i)\n",
        "        return result\n",
        "\n",
        "    def inverse_transform(self,data):\n",
        "        result=data.copy(deep=True)\n",
        "        for i in range(0,len(self.classes)):\n",
        "            result=result.replace(i,self.classes[i])\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "84919f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84919f0d",
        "outputId": "8c8a4fee-d25d-476c-9e18-5cc84ad9afce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IterativeImputer] Completing matrix with shape (59299, 20)\n",
            "[IterativeImputer] Change: 84.68516837832195, scaled tolerance: 47.466 \n",
            "[IterativeImputer] Change: 32.199999999999996, scaled tolerance: 47.466 \n",
            "[IterativeImputer] Early stopping criterion reached.\n",
            "[IterativeImputer] Completing matrix with shape (59299, 20)\n",
            "[IterativeImputer] Completing matrix with shape (5271, 20)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "imputer=IterativeImputer(estimator=RandomForestRegressor(random_state=SEED),initial_strategy='most_frequent',max_iter=10,random_state=SEED,skip_complete=False,verbose=1)\n",
        "\n",
        "# encoder\n",
        "encoders={}\n",
        "for object_column in object_columns:\n",
        "    encoders[object_column]=Encoder()\n",
        "    encoders[object_column].fit(train_process[object_column])\n",
        "\n",
        "\n",
        "def order_encoding(target_data,object_columns,encoders):\n",
        "    result=pd.DataFrame()\n",
        "    for col in target_data.columns:\n",
        "        if col in object_columns:\n",
        "            result[col]=encoders[col].transform(target_data[col])\n",
        "        else:\n",
        "            result[col]=target_data[col]\n",
        "\n",
        "    return result\n",
        "\n",
        "def order_decoding(target_data,object_columns,encoders):\n",
        "    result=pd.DataFrame()\n",
        "    for col in target_data.columns:\n",
        "        if col in object_columns:\n",
        "            result[col]=target_data[col].apply(lambda x:round(x)).astype(int)\n",
        "            result[col]=encoders[col].inverse_transform(result[col])\n",
        "        else:\n",
        "            result[col]=target_data[col]\n",
        "    return result\n",
        "\n",
        "# train data\n",
        "train_dummy=order_encoding(train_process,object_columns,encoders)\n",
        "imputer.fit(train_dummy)\n",
        "train_dummy_imputed=pd.DataFrame(data=imputer.transform(train_dummy),columns=train_process.columns)\n",
        "train_imputed=order_decoding(train_dummy_imputed,object_columns,encoders)\n",
        "\n",
        "# test data\n",
        "test_dummy=order_encoding(test_process,object_columns,encoders)\n",
        "test_dummy_imputed=pd.DataFrame(data=imputer.transform(test_dummy),columns=test_process.columns)\n",
        "test_imputed=order_decoding(test_dummy_imputed,object_columns,encoders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4d2a2062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d2a2062",
        "outputId": "66eab8fe-3ecf-4f63-c923-ac3f5c25d108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-2 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-3 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-4 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-5 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-6 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-7 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-8 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-9 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-10 fold data-\n",
            "X data shape: (9800, 20)\n",
            "y data shape: (9800,)\n",
            "-11 fold data-\n",
            "X data shape: (9799, 20)\n",
            "y data shape: (9799,)\n",
            "test data shape: (5271, 20)\n"
          ]
        }
      ],
      "source": [
        "# data kfold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "train_datas=[]\n",
        "train_imputed['is_converted']=targets\n",
        "train_data_false=train_imputed[train_imputed['is_converted']==0]\n",
        "train_data_true=train_imputed[train_imputed['is_converted']==1]\n",
        "\n",
        "# kfold\n",
        "K=11\n",
        "dkf=KFold(n_splits=K,shuffle=True,random_state=SEED)\n",
        "preserves=[pd.DataFrame() for x in range(0,K+1)]\n",
        "for i,(_,index) in enumerate(dkf.split(train_data_false)):\n",
        "    print(f'-{i+1} fold data-')\n",
        "    y_data_false=train_data_false['is_converted'].iloc[index]\n",
        "    X_data_false=train_data_false.drop('is_converted',axis=1).iloc[index]\n",
        "\n",
        "    y_data=pd.concat([y_data_false,train_data_true['is_converted']],ignore_index=True)\n",
        "    X_data=pd.concat([X_data_false,train_data_true.drop('is_converted',axis=1)],ignore_index=True)\n",
        "\n",
        "    # preserve\n",
        "    # preserves[i]['com_reg_ver_win_rate']=X_data['com_reg_ver_win_rate']\n",
        "    # X_data=X_data.drop('com_reg_ver_win_rate',axis=1)\n",
        "    # preserves[i]['customer_idx']=X_data['customer_idx']\n",
        "    # X_data['customer_idx']=np.log1p(X_data['customer_idx'])\n",
        "    # preserves[i]['lead_owner']=X_data['lead_owner']\n",
        "    # X_data['lead_owner']=np.log1p(X_data['lead_owner'])\n",
        "\n",
        "\n",
        "    print(f'X data shape: {X_data.shape}')\n",
        "    print(f'y data shape: {y_data.shape}')\n",
        "    train_datas.append((X_data,y_data))\n",
        "\n",
        "# preserve\n",
        "# preserves[K]['com_reg_ver_win_rate']=test_process['com_reg_ver_win_rate']\n",
        "# test_data=test_process.drop('com_reg_ver_win_rate',axis=1)\n",
        "# preserves[K]['customer_idx']=X_data['customer_idx']\n",
        "# test_data['customer_idx']=np.log1p(test_data['customer_idx'])\n",
        "# preserves[K]['lead_owner']=X_data['lead_owner']\n",
        "# test_data['lead_owner']=np.log1p(test_data['lead_owner'])\n",
        "test_data=test_imputed\n",
        "print(f'test data shape: {test_data.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type_value_counts = test_data['customer_type'].value_counts()\n",
        "print(type_value_counts)\n",
        "\n",
        "country_value_counts = test_data['customer_country'].value_counts()\n",
        "print(country_value_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI2TVwrNpxtW",
        "outputId": "115b76a0-794e-4de2-fe02-19ce49be8089"
      },
      "id": "WI2TVwrNpxtW",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EC    2826\n",
            "SI    1661\n",
            "CP     539\n",
            "OT     245\n",
            "Name: customer_type, dtype: int64\n",
            "brazil                  1045\n",
            "india                    906\n",
            "unitedstates             782\n",
            "philippines              289\n",
            "OT                       254\n",
            "peru                     211\n",
            "mexico                   180\n",
            "colombia                 127\n",
            "u.a.e                    120\n",
            "italy                    120\n",
            "chile                    108\n",
            "saudiarabia              102\n",
            "indonesia                 95\n",
            "egypt                     95\n",
            "vietnam                   69\n",
            "germany                   68\n",
            "australia                 66\n",
            "argentina                 65\n",
            "türkiye                   51\n",
            "hongkong                  48\n",
            "unitedkingdom             43\n",
            "poland                    42\n",
            "singapore                 41\n",
            "thailand                  38\n",
            "panama                    27\n",
            "serbia                    20\n",
            "greece                    19\n",
            "southafrica               19\n",
            "canada                    17\n",
            "oman                      16\n",
            "france                    16\n",
            "spain                     15\n",
            "ecuador                   14\n",
            "hungary                   13\n",
            "costarica                  8\n",
            "nigeria                    8\n",
            "romania                    8\n",
            "venezuela                  8\n",
            "algeria                    7\n",
            "honduras                   7\n",
            "dominicanrepublic          6\n",
            "guatemala                  6\n",
            "bulgaria                   6\n",
            "belgium                    5\n",
            "china                      5\n",
            "qatar                      5\n",
            "elsalvador                 4\n",
            "portugal                   4\n",
            "puertorico                 3\n",
            "kenya                      3\n",
            "afghanistan                3\n",
            "jordan                     3\n",
            "nicaragua                  3\n",
            "cambodia                   2\n",
            "kuwait                     2\n",
            "bahrain                    2\n",
            "bolivia                    2\n",
            "netherlands                2\n",
            "malaysia                   2\n",
            "iraq                       2\n",
            "malta                      2\n",
            "maldives                   2\n",
            "southkorea                 1\n",
            "senegal                    1\n",
            "paraguay                   1\n",
            "jamaica                    1\n",
            "uruguay                    1\n",
            "bangladesh                 1\n",
            "switzerland                1\n",
            "bosniaandherzegovina       1\n",
            "israel                     1\n",
            "pakistan                   1\n",
            "Name: customer_country, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "6facec60",
      "metadata": {
        "id": "6facec60"
      },
      "outputs": [],
      "source": [
        "# process data type\n",
        "for (train_data,target) in train_datas:\n",
        "    train_data['customer_idx']=train_data['customer_idx'].astype(str)\n",
        "    train_data['lead_owner']=train_data['lead_owner'].astype(str)\n",
        "    train_data['strategic_ver']=train_data['strategic_ver'].astype(int)\n",
        "    train_data['grant_weight']=train_data['grant_weight'].astype(int)\n",
        "    target=target.apply(lambda x:1 if x else 0)\n",
        "\n",
        "test_data['customer_idx']=test_data['customer_idx'].astype(str)\n",
        "test_data['lead_owner']=test_data['lead_owner'].astype(str)\n",
        "test_data['strategic_ver']=test_data['strategic_ver'].astype(int)\n",
        "test_data['grant_weight']=test_data['grant_weight'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f9603690",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9603690",
        "outputId": "e23bdd1a-ef5b-4b89-8895-2ae95f29f6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.3-cp310-cp310-manylinux2014_x86_64.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.3\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics         import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "55ec1eac",
      "metadata": {
        "id": "55ec1eac"
      },
      "outputs": [],
      "source": [
        "# 모델 성능 테스트\n",
        "def get_clf_eval(y_test, y_pred=None):\n",
        "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
        "\n",
        "    print(\"오차행렬:\\n\", confusion)\n",
        "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
        "    print(\"정밀도: {:.4f}\".format(precision))\n",
        "    print(\"재현율: {:.4f}\".format(recall))\n",
        "    print(\"F1: {:.4f}\".format(F1))\n",
        "    return F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0c736ead",
      "metadata": {
        "id": "0c736ead"
      },
      "outputs": [],
      "source": [
        "class KMODEL:\n",
        "    def __init__(self,dataset_K,train_K=5):\n",
        "        self.k_data=dataset_K\n",
        "        self.k_fold=train_K\n",
        "        self.models=[[] for i in range(0,K)]\n",
        "        self.scores=[[] for i in range(0,K)]\n",
        "        self.thresholds=[[] for i in range(0,K)]\n",
        "        self.cv_scores=[]\n",
        "\n",
        "    def modeling_kfold(self,iters,n_estimators,max_depth,learning_rate,cat_features,train_data,targets_data,core):\n",
        "        # k-fold\n",
        "        kf=StratifiedKFold(n_splits=self.k_fold,shuffle=True,random_state=SEED)\n",
        "\n",
        "        for i,(train_index,val_index) in enumerate(kf.split(train_data,targets_data)):\n",
        "            print(f'-[{iters+1}-{i+1}] fold-')\n",
        "            X_train,X_val=train_data.iloc[train_index],train_data.iloc[val_index]\n",
        "            y_train,y_val=targets_data.iloc[train_index],targets_data.iloc[val_index]\n",
        "\n",
        "            # logloss\n",
        "            classifier=CatBoostClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, eval_metric='F1', random_state=SEED, bootstrap_type ='Bernoulli',task_type=core) # default:logloss\n",
        "\n",
        "            # randomforst\n",
        "            #classifier=RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,random_state=SEED)\n",
        "\n",
        "            # regressor=CatBoostRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, eval_metric='RMSE',random_state=SEED, bootstrap_type ='Bernoulli',task_type=core)\n",
        "\n",
        "            model=classifier.fit(X_train, y_train, eval_set=(X_val,y_val),verbose=100, early_stopping_rounds=100,cat_features=cat_features,use_best_model=True)\n",
        "            #model=classifier.fit(X_train,y_train)\n",
        "            # model=regressor.fit(X_train, y_train, eval_set=(X_val,y_val),verbose=100, early_stopping_rounds=100,cat_features=cat_features)\n",
        "\n",
        "\n",
        "            # pred=model.predict(X_val,prediction_type='RawFormulaVal')  # focal loss\n",
        "            # coordinates=np.linspace(pred.min(),pred.max(),100)   # focal loss\n",
        "            pred=model.predict_proba(X_val)[:,1]\n",
        "            coordinates = np.linspace(0, 1, 100)\n",
        "\n",
        "            best_score=0\n",
        "            best_coordinate=0\n",
        "            for coordinate in coordinates:\n",
        "                pred_value=pred>coordinate\n",
        "                score=f1_score(y_val,pred_value)\n",
        "                if best_score<score:\n",
        "                    best_score=score\n",
        "                    best_coordinate=coordinate\n",
        "\n",
        "            pred=(pred>best_coordinate)\n",
        "            self.scores[iters].append(get_clf_eval(y_val,pred))\n",
        "            self.thresholds[iters].append(best_coordinate)\n",
        "            # scores.append(np.sqrt(mean_squared_error(y_val,pred)))\n",
        "            self.models[iters].append(model)\n",
        "\n",
        "        self.cv_scores.append(np.mean(self.scores[iters]))\n",
        "        print(f'[{iters+1}] F1 scores mean: {self.cv_scores[iters]}')\n",
        "\n",
        "    def modeling_kdata(self,n_estimators,max_depth,learning_rate,cat_features,train_datas,core='CPU'):\n",
        "        for iter,(train_data,target) in enumerate(train_datas):\n",
        "            self.modeling_kfold(iter,n_estimators,max_depth,learning_rate,cat_features,train_data,target,core=core)\n",
        "        print(f'Total F1 scores mean: {np.mean(self.cv_scores)}')\n",
        "\n",
        "    def predict(self,test_data):\n",
        "        test_pred=pd.Series([0 for x in range(len(test_data))], index=test_data.index)\n",
        "        for models,thresholds in zip(self.models,self.thresholds):\n",
        "            for model,threshold in zip(models,thresholds):\n",
        "                pred=model.predict_proba(test_data)[:,1]\n",
        "                test_pred+=(pred>threshold)\n",
        "        test_pred=test_pred/(self.k_data*self.k_fold)\n",
        "        test_pred=test_pred.apply(lambda x:1 if x>0.5 else 0)\n",
        "        return test_pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e29d38b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e29d38b6",
        "outputId": "31b46fcd-fd1d-414e-b17b-9d8b15943d53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['customer_country',\n",
              " 'business_unit',\n",
              " 'customer_idx',\n",
              " 'customer_type',\n",
              " 'enterprise',\n",
              " 'customer_job',\n",
              " 'inquiry_type',\n",
              " 'product_category',\n",
              " 'customer_position',\n",
              " 'response_corporate',\n",
              " 'expected_timeline',\n",
              " 'business_area',\n",
              " 'lead_owner',\n",
              " 'strategic_ver',\n",
              " 'region',\n",
              " 'grant_weight']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "cat_features=train_datas[0][0].columns.to_list()\n",
        "cat_features.remove('bant_submit')\n",
        "cat_features.remove('lead_desc_length')\n",
        "cat_features.remove('historical_existing_cnt')\n",
        "cat_features.remove('com_reg_ver_win_rate')\n",
        "cat_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "34fd1e3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "34fd1e3f",
        "outputId": "7b014208-81b3-4ca2-d74d-c25ab6724cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-[1-1] fold-\n",
            "0:\tlearn: 0.8409231\ttest: 0.8518519\tbest: 0.8518519 (0)\ttotal: 107ms\tremaining: 1m 47s\n",
            "100:\tlearn: 0.9597848\ttest: 0.9282103\tbest: 0.9287519 (90)\ttotal: 11s\tremaining: 1m 38s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d8b8e2a4749f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKMODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_K\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_kdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-f216a1570e9c>\u001b[0m in \u001b[0;36mmodeling_kdata\u001b[0;34m(self, n_estimators, max_depth, learning_rate, cat_features, train_datas, core)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodeling_kdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Total F1 scores mean: {np.mean(self.cv_scores)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-f216a1570e9c>\u001b[0m in \u001b[0;36mmodeling_kfold\u001b[0;34m(self, iters, n_estimators, max_depth, learning_rate, cat_features, train_data, targets_data, core)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# regressor=CatBoostRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, eval_metric='RMSE',random_state=SEED, bootstrap_type ='Bernoulli',task_type=core)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m#model=classifier.fit(X_train,y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# model=regressor.fit(X_train, y_train, eval_set=(X_val,y_val),verbose=100, early_stopping_rounds=100,cat_features=cat_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5199\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5201\u001b[0;31m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[1;32m   5202\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5203\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2395\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2396\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2397\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "kmodel=KMODEL(dataset_K=11)\n",
        "kmodel.modeling_kdata(n_estimators=1000,max_depth=10,learning_rate=0.05,cat_features=cat_features,train_datas=train_datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af54d90",
      "metadata": {
        "id": "9af54d90"
      },
      "outputs": [],
      "source": [
        "pred=kmodel.predict(test_data)\n",
        "pred.value_counts()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}